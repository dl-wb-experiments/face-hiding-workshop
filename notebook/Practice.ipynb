{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Agenda\r\n",
    "\r\n",
    "## 1. [Introduction](#s1)\r\n",
    "\r\n",
    "## 2. [OpenVINO™ Overview](#s6) - Kashchikhin\r\n",
    "\r\n",
    "## 3. [OpenVINO™ Deep Learning Workbench](#s7) - Kashchikhin\r\n",
    "\r\n",
    "## 4. [OpenVINO(TM) API](#s7) – Tugaryov\r\n",
    "\r\n",
    "Object Detection sample: http://127.0.0.1:5665/jupyter/lab/tree/tutorials/object_detection_ssd/tutorial_object_detection_ssd.ipynb\r\n",
    "W/o downloader and w/updated cells\r\n",
    "\r\n",
    "## 5. [Practice](#s15) – Tugaryov\r\n",
    "\r\n",
    "Task 1: apply pre-defined blur method to given image at inferred coordinates (photo with several faces)\r\n",
    "\r\n",
    "Task 2: add blurring logic to pre-defined video processor\r\n",
    "\r\n",
    "Task 3: replace each face on the photo with a smile with corresponding emotion\r\n",
    "\r\n",
    "Task 4: emotional smile on the video\r\n",
    "\r\n",
    "** Task 5: Telegram Bot - Smiler"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Intro\r\n",
    "\r\n",
    "### What You Will Learn\r\n",
    "\r\n",
    "Welcome to the Intel workshop where you will find out how to start working with pre-trained neural networks and write your AI application. For that, we will use OpenVINO™ framework and its graphical interface Deep Learning Workbench. \r\n",
    "\r\n",
    "During this workshop you will:\r\n",
    "\r\n",
    "1. Learn the basics of neural model analysis and optimization:\r\n",
    "    - what a model is and how it works\r\n",
    "    - how to measure its performance and analyze the quality\r\n",
    "    - how to tune the model for enhanced performance\r\n",
    "2. Write your own AI application that detects faces and recognizes emotions on video\r\n",
    "\r\n",
    "We have prepared this workshop to make your Deep Learning journey easy and exciting. Hope you find it useful and !!looking forward for your feedback!!. \r\n",
    "\r\n",
    "### Why Deep Learning\r\n",
    "\r\n",
    "Deep Learning recently has gained wide popularity due to significant breakthroughs in the artificial neural networks area. From digital assistants and chatbots in customer service to object recognition in retail, Deep learning has enabled the development of various new AI applications. With Deep Learning, the algorithm does not need to be taught about the essential features. It can discover features from data on its own using a neural network. Generally, Deep Learning algorithms use massive amounts of data and aim to simulate the human brain capacity to observe, learn, and make decisions, particularly for highly complex tasks. \r\n",
    "\r\n",
    "### Inference vs Training\r\n",
    "\r\n",
    "To perform a specific AI task, the model is firstly trained on the known data. During training, the model makes predictions about what the data represents. Any error in the prediction is used to strengthen the artificial neuron connections until the acceptable accuracy level is achieved. The process of training usually happens only once, requires massive amount of data and powerful computing systems. \r\n",
    "The process of using a pre-trained model to make predictions against previously unseen data is called inference. Inference is performed on different devices, happens several times and consists in feeding the images to the model. In this workshop we will work with model inference, in other words, we will execute the model already pre-trained to perform useful work.\r\n",
    "\r\n",
    "![](pictures/deep-learning.png)\r\n",
    "\r\n",
    "Author: Mark Robins, Intel, [Link](https://www.intel.ru/content/www/ru/ru/artificial-intelligence/posts/deep-learning-training-and-inference.html)\r\n",
    "\r\n",
    "\r\n",
    "Let's take a look at the inference on the real-life example of vehical detection model. The model running alongside the traffic camera is continuously processing a video to identify the cars approaching the intersection. When a car enters on the red light, several photos of this car are taken and provided to the model, which locates a license plate on the image and sends it for further processing.\r\n",
    "At the server, a first inference is run to localize the license plate in the image, and a second inference is run to read the characters on the license plate. After this, the license plate information is sent to the data center, where an application checks for potential traffic violations. \r\n",
    "\r\n",
    "| Device| Data transmission costs | Power consumption|Сomputing resource|\r\n",
    "|:----- | :----- | :----- | :-----|\r\n",
    "|Traffic Camera   | Low| Low | Low |\r\n",
    "| Gateway Server| Average| Average| Average |\r\n",
    "|Data Center | High| High| Powerful|\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "![](pictures/system.png)\r\n",
    "\r\n",
    "Author: Mark Robins, Intel, [Link](https://www.intel.ru/content/www/ru/ru/artificial-intelligence/posts/deep-learning-training-and-inference.html)\r\n",
    "\r\n",
    "Therefore, given the specific task and the cost of data transmission, it is often beneficial to bring the model inference closer to the target where the data was primarily received. Let's now find out how the OpenVINO™ toolkit can help in achieving better performance of the Deep Learning models. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. OpenVINO™ Toolkit"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The OpenVINO™ toolkit is a comprehensive toolkit for optimizing pretrained deep learning models of various use cases to achieve high performance and prepare them for deployment on Intel® platforms. Based on latest generations of artificial neural networks, including Convolutional Neural Networks (CNNs), recurrent and attention-based networks, the toolkit extends computer vision and non-vision workloads across Intel® hardware, maximizing performance. It accelerates applications with high-performance, AI and Deep Learning inference deployed from edge to cloud."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1 Introducing the OpenVINO™ toolkit"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](pictures/about_vino.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 OpenVINO™ Capabilities"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](pictures/openvino_toolkit.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3 OpenVINO™ Toolkit Components"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](pictures/additional_tools.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.4 OpenVINO™ Deep Learning Models\r\n",
    "\r\n",
    "A model is a network that has been trained over a set of data using a certain framework. Since Deep learning technologies are used in various industrial\r\n",
    "applications, it is crucial to have an effective solution for each specific use case. OpenVINO Open Model Zoo provides a range of public and Intel pre-trained models to resolve a variety of different tasks, such as classification, object detection, segmentation and many others."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](pictures/models.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## OpenVINO (Propaganda)\n",
    "\n",
    "![](pictures/ov-repository.png)\n",
    "\n",
    "[Link](https://github.com/openvinotoolkit/openvino)\n",
    "\n",
    "-----------------\n",
    "\n",
    "![](pictures/ov-coursera.png)\n",
    "\n",
    "[Link](https://www.coursera.org/learn/intel-openvino)\n",
    "\n",
    "-------------------\n",
    "\n",
    "![](pictures/ov-summit.png)\n",
    "\n",
    "[Source](https://blogs.intel.com/psg/openvino-toolkit-wins-vision-product-of-the-year-award-in-best-developer-tools-category-at-embedded-vision-summit/)\n",
    "\n",
    "--------------------\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Deep Learning Workbench: OpenVINO™ Quickstart"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Deep Learning Workbench (DL Workbench) is the official OpenVINO™ graphical interface designed to make the production of pretrained deep learning models significantly easier.\r\n",
    "With DL Workbench you can start working with your deep learning model right from your browser: import a model, analyze its performance and accuracy, visualize the outputs, optimize and prepare the model for deployment in a matter of minutes. \r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](pictures/openvino_toolkit-dl-wb-highlighted.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.1 DL Workbench Capabilities"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](pictures/openvino_dl_wb.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "By now you have learned about different tools that help accelerate your model. But that's enough of the theory; let's get down to the practice. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 DL Workbench Workflow\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2.1 Open DL Workbench\r\n",
    "\r\n",
    "To start working with DL Workbench click **Create** button to open Create Project page.\r\n",
    "\r\n",
    "![](pictures/create_project_first_page.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2.2 Import the Model\r\n",
    "\r\n",
    "The first step is to import our model. Select and import face-detection-adas-0001 model from the Open Model Zoo. \r\n",
    "\r\n",
    "![](pictures/Import_model_button.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](pictures/Import_Model.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2.3 Import Dataset\r\n",
    "\r\n",
    "You will need data to work with the model. The data can be in different formats depending on a task the model is trained to perform. You can learn about these formats in the [documentation](https://docs.openvinotoolkit.org/latest/workbench_docs_Workbench_DG_Dataset_Types.html). \r\n",
    "In our case, we will take a set of images and use them as the validation dataset. \r\n",
    "1. Use the following link to download the dataset: [Link](https://github.com/dl-wb-experiments/face-hiding-workshop/files/7043878/dataset.zip).\r\n",
    "2. Unarchive it;\r\n",
    "3. Go to DL Workbench. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](pictures/dataset_select.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Drag and drop the images from the archive to create a dataset and click **Import**.\r\n",
    "\r\n",
    "![](pictures/dataset_import.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2.4 Benchmark the Model\r\n",
    "\r\n",
    "We have imported our model and now want to check how fast it is. For that, let's create our first project. Select the model and the dataset by clicking on them. You can also choose a hardware accelerator on which a model will be executed. We will analyze how our model works on CPU since we have only this device available. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](pictures/Project_configured.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2.5 Analyze the Model\r\n",
    "\r\n",
    "When the inference stage is finished, we can see the result that we get after running our model on the CPU. Latency is the time required to process one image. The lower the value, the better. Throughput is the number of images (frames) processed per second. Higher throughput value means better performance. Let's check how the model works and try to make it even faster."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](pictures/analyze.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To test how our model works, you can select an image from the downloaded dataset, upload this image and visualize the model predictions. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](pictures/predictions.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2.6 Optimize the Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "One of the recommended ways to accelerate your model performance is to perform 8-bit integer (INT8) calibration.\r\n",
    "A model in the INT8 precision takes up less memory and has higher throughput capacity. INT8 calibration is a universal method to speed up the model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](pictures/int8-page.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Analyze the Improvements\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](pictures/dashboard-parent-vs-optimized.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2.7 Profile the Model\r\n",
    "\r\n",
    "We can observe that after calibration our model became faster. But that's not all. You can speed up your model even further if you select a set of appropriate parameters specific to each accelerator: streams and batches. In simple terms, streams is the number of cores and batches is the amount of images fed to the model. \r\n",
    "\r\n",
    "![](pictures/batch_stream_diagram.png)\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](pictures/explore-inference.png)\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](pictures/inference-table.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let’s recap briefly what you have learned at this stage:\r\n",
    "\r\n",
    "1. What a model is and how it works\r\n",
    "2. How to measure its performance\r\n",
    "3. How to accelerate the model using INT8 calibration\r\n",
    "4. How different options affect model performance \r\n",
    "\r\n",
    "Our next step is to apply this knowledge to build the face detection and emotion recognition application. Before we proceed, we should determine our model location. For that, go to the Learn OpenVINO tab, select Model Inference with OpenVINO API and copy the model path."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Copy Paths to the Model\n",
    "TODO: Move to the next section"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](pictures/model-paths.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# OpenVINO™ API"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's learn how to infer a model of object detection use case with OpenVINO™ Python interface and build our application. Object Detection in Computer Vision is a task of finding objects of a certain class and highlighting them with bounding boxes.\r\n",
    "\r\n",
    "We will go through the following steps:\r\n",
    "\r\n",
    "1. [Import required modules](#1.-Import-Required-Modules) \r\n",
    "3. [Configure inference: path to a model and other data](#3.-Configure-an-Inference)\r\n",
    "4. [Initialize the OpenVINO™ runtime](#4.-Initialize-the-OpenVINO™-Runtime)\r\n",
    "5. [Read the model](#5.-Read-the-Model)\r\n",
    "6. [Make the model executable](#6.-Make-the-Model-Executable)\r\n",
    "7. [Prepare an image for model inference](#7.-Prepare-an-Image-for-Model-Inference)\r\n",
    "8. [Infer the model](#8.-Infer-the-Model)\r\n",
    "9. [Show predictions](#9.-Show-Predictions)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Import Required Modules\r\n",
    "\r\n",
    "Import the Python* modules that you will use in the sample code:\r\n",
    "- [pathlib](https://docs.python.org/3/library/os.html#module-os) is a standard Python module used for filename parsing\r\n",
    "- [cv2](https://docs.opencv.org/trunk/) is an OpenCV module used to work with images\r\n",
    "- [NumPy](http://www.numpy.org/) is an array manipulation module used to process images as arrays\r\n",
    "- [OpenVINO Inference Engine](https://docs.openvinotoolkit.org/latest/openvino_docs_IE_DG_Deep_Learning_Inference_Engine_DevGuide.html) is an OpenVINO™ Python API module used for inference\r\n",
    "- [IPython](https://ipython.readthedocs.io/en/stable/index.html) is an IPython API uused for showing images and videos in the notebook\r\n",
    "\r\n",
    "Run the cell below to import the modules. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\r\n",
    "import cv2\r\n",
    "import numpy as np\r\n",
    "from openvino.inference_engine import IECore\r\n",
    "from IPython.display import HTML, Image, display"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Evaluate Performance\r\n",
    "\r\n",
    "Once you have the OpenVINO™ IR of your model, you can start experimenting with it by inferring it and inspecting its output. \r\n",
    "\r\n",
    "> **NOTE**: Copy the paths to the `.xml` and `.bin` files from the DL Workbench UI and paste them below.\r\n",
    "#### Required parameters\r\n",
    "\r\n",
    "Parameter| Explanation\r\n",
    "---|---\r\n",
    "**model_xml**| Path to the `.xml` file of OpenVINO™ IR of your model\r\n",
    "**model_bin**| Path to the `.bin` file of OpenVINO™ IR of your model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Model IR files\r\n",
    "face_detection_model_xml = 'data/models/face-detection-adas-0001.xml'\r\n",
    "face_detection_model_bin = 'data/models/face-detection-adas-0001.bin'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Optional Parameters\n",
    "\n",
    "Experiment with optional parameters after you go the full workflow of the tutorial.\n",
    "\n",
    "Parameter| Explanation\n",
    "---|---\n",
    "**input_image_path**| Path to an input image. Use the `car.bmp` image placed in the directory of the notebook or, if you have imported a dataset in the DL Workbench, copy the path to an image in the dataset.\n",
    "**device**| Specify the [target device](https://docs.openvinotoolkit.org/latest/workbench_docs_Workbench_DG_Select_Environment.html) to infer on: CPU, GPU, or MYRIAD. Note that the device must be present. For this tutorial, use `CPU` which is known to be present.\n",
    "**prob_threshold**| Probability threshold to filter detection results - PERCENT!!!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Input image file. \r\n",
    "input_image_path = 'data/input_image.JPG'\r\n",
    "\r\n",
    "# Input video file\r\n",
    "input_video_path = 'data/input.mp4'\r\n",
    "\r\n",
    "# Output video file\r\n",
    "output_video_path = 'data/output.mp4'\r\n",
    "\r\n",
    "# Device to use\r\n",
    "device = 'CPU'\r\n",
    "\r\n",
    "# Minimum percentage threshold to detect an object\r\n",
    "prob_threshold = 50\r\n",
    "\r\n",
    "print(\r\n",
    "f'''Configuration parameters settings:\r\n",
    "    model_xml={face_detection_model_xml},\r\n",
    "    model_bin={face_detection_model_bin},\r\n",
    "    input_image_path={input_image_path},\r\n",
    "    device={device}, \r\n",
    "    prob_threshold={prob_threshold}''',\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Initialize the OpenVINO™ Runtime\n",
    "\n",
    "Once you define the parameters, let's initiate the `IECore` object that accesses OpenVINO™ runtime capabilities."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create an Inference Engine instance\r\n",
    "ie_core = IECore()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. Read the Model\n",
    "\n",
    "Put the IR of your model in the memory."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Read the network from IR files\r\n",
    "face_detection_network = ie_core.read_network(model=face_detection_model_xml, weights=face_detection_model_bin)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5. Make the Model Executable\r\n",
    "\r\n",
    "Reading a model is not enough to start a model inference. The model must be loaded to a particular abstraction representing a particular accelerator. In OpenVINO™, this abstraction is called *plugin*. A network loaded to a plugin becomes executable and will be inferred in one of the next steps. \r\n",
    "\r\n",
    "After loading, we keep necessary model information such as names of input and output blobs: `input_blob` and `output_blob`. Let's remember the input dimensions of your model:\r\n",
    "- `n` - input batch size\r\n",
    "- `c` - number of input channels. Often, it is `1` or `3`, which means that the model expects either a grayscale or a color image.\r\n",
    "- `h` - input image height\r\n",
    "- `w` - input image width"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Store names of input and output blobs\r\n",
    "face_detector_input_name = next(iter(face_detector.input_info))\r\n",
    "face_detector_output = next(iter(face_detection_network.outputs))\r\n",
    "\r\n",
    "# Read the input dimensions: n=batch size, c=number of channels, h=height, w=width\r\n",
    "face_detection_network_input_shape = face_detection_network.input_info[face_detection_input_blob].input_data.shape\r\n",
    "n, c, face_detector_input_height, face_detection_input_width = face_detection_network_input_shape\r\n",
    "print(f'Face Detection model input dimensions: n={face_detection_input_batch}, c={face_detection_input_channels}, h={face_detection_input_height}, w={face_detection_input_width}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6. Load the Model to the Device\r\n",
    "This step is necessary to infer the model. You can learn more in the [documentation](https://docs.openvinotoolkit.org/latest/ie_python_api/classie__api_1_1IECore.html#ac9a2e043d14ccfa9c6bbf626cfd69fcc)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(f'Loaded the model into the Inference Engine for the {device} device.'), \r\n",
    "face_detection_executable_network = ie_core.load_network(network=face_detection_network, device_name=device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 7. Prepare an Image for Model Inference\r\n",
    "\r\n",
    "Now let's read and prepare the input image by resizing and re-arranging its dimensions according to the input dimensions of the model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Define the function to load the input image\r\n",
    "def load_input_image(input_path):   \r\n",
    "    # Use OpenCV to load the input image\r\n",
    "    image = cv2.imread(input_path)\r\n",
    "    return image\r\n",
    "\r\n",
    "# Define the function to pre-process (resize, transpose) the input image\r\n",
    "def pre_process_input_image(image: np.ndarray, target_height: int, target_width: int) -> np.ndarray:\r\n",
    "    # Resize the image dimensions from image to model input w x h\r\n",
    "    resized_image = cv2.resize(image, (target_width, target_height))\r\n",
    "    \r\n",
    "    # Change data layout from HWC to CHW\r\n",
    "    transposed_image = resized_image.transpose((2, 0, 1))\r\n",
    "    \r\n",
    "    n = 1 # Batch is always 1 in our case\r\n",
    "    c = 3 # Channels is always 3 in our case\r\n",
    "    \r\n",
    "    # Reshape to input dimensions\r\n",
    "    reshaped_image = transposed_image.reshape((n, c, target_height, target_width))\r\n",
    "    return reshaped_image\r\n",
    "\r\n",
    "def show_image(image: np.ndarray):\r\n",
    "    _, data = cv2.imencode('.jpg', image) \r\n",
    "    image = Image(data=data)\r\n",
    "    display(image)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Use OpenCV to load the input image\r\n",
    "original_image = cv2.imread(input_image_path)\r\n",
    "\r\n",
    "# Prepare the image\r\n",
    "input_frame = pre_process_input_image(original_image, target_height=face_detection_input_height, target_width=face_detection_input_width)\r\n",
    "\r\n",
    "# Display the input image\r\n",
    "show_images(original_image)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 8. Infer the Model\r\n",
    "Now we proceed to inference and feed the prepared image to the model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "face_detection_inference_results = face_detection_executable_network.infer(\r\n",
    "    inputs={\r\n",
    "        face_detection_input_blob: input_frame\r\n",
    "    }\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A model can have many outputs, so the `infer` method returns the dictionary, where the keys are the names of output layers and the values - the results of inference for each output layer."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "face_detection_inference_results = face_detection_inference_results[face_detection_output_blob]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 9. Show Predictions\r\n",
    "\r\n",
    "The next step is to parse the inference results and draw boxes over the objects detected in the image.\r\n",
    "\r\n",
    "The result of model inference (`face_detection_inference_results`) is an array of predictions. Each prediction `object` has the following structure:\r\n",
    "\r\n",
    "- `object[2]`: confidence level that the detected object is an instance of the predicted class\r\n",
    "- `object[3]`: lower x coordinate of the detected object \r\n",
    "- `object[4]`: lower y coordinate of the detected object\r\n",
    "- `object[5]`: upper x coordinate of the detected object\r\n",
    "- `object[6]`: upper y coordinate of the detected object"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def parse_face_detection_results(inference_results: np.ndarray, original_image_width: int, original_image_height: int) -> list:\r\n",
    "    detected_faces = []\r\n",
    "    \r\n",
    "    for inference_result in inference_results[0][0]:\r\n",
    "        confidence = round(inference_result[2] * 100, 1)\r\n",
    "\r\n",
    "        # If confidence is more than the specified threshold, draw and label the box \r\n",
    "        if confidence > prob_threshold:\r\n",
    "\r\n",
    "            # Get coordinates of the box containing the detected object\r\n",
    "            xmin = int(inference_result[3] * original_image_width)\r\n",
    "            ymin = int(inference_result[4] * original_image_height)\r\n",
    "            xmax = int(inference_result[5] * original_image_width)\r\n",
    "            ymax = int(inference_result[6] * original_image_height)\r\n",
    "\r\n",
    "            detected_face = (xmin, ymin, xmax, ymax, confidence)\r\n",
    "            detected_faces.append(detected_face)\r\n",
    "            \r\n",
    "    return detected_faces"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](pictures/models_diagram.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Function to process inference results\r\n",
    "def post_process_face_detection(inference_results: np.ndarray, original_image: np.ndarray) -> np.ndarray:\r\n",
    "    original_image_height, original_image_width, *_ = original_image.shape\r\n",
    "    \r\n",
    "    processed_image = original_image.copy()\r\n",
    "    \r\n",
    "    # Get output results\r\n",
    "    color = (12.5, 255, 255)\r\n",
    "    \r\n",
    "    detected_faces = parse_face_detection_results(inference_results, original_image_width, original_image_height)\r\n",
    "    # Loop through all possible results\r\n",
    "    for detected_face in detected_faces:\r\n",
    "        xmin, ymin, xmax, ymax, confidence = detected_face\r\n",
    "\r\n",
    "        # Draw the box and label for the detected object\r\n",
    "        cv2.rectangle(processed_image, (xmin, ymin), (xmax, ymax), color, 4)\r\n",
    "        cv2.putText(processed_image, \r\n",
    "                    f'{confidence} %', (xmin, ymin - 7), \r\n",
    "                    cv2.FONT_HERSHEY_COMPLEX, 1, color, 2)\r\n",
    "            \r\n",
    "    return processed_image"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "processed_image = post_process_face_detection(face_detection_inference_results, original_image)\r\n",
    "\r\n",
    "show_images(processed_image)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Practice"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 1: Apply pre-defined blur method to the image"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\r\n",
    "Define a function to blur an image."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def blur(image: np.ndarray) -> np.ndarray:\r\n",
    "    height, width = image.shape[:2]\r\n",
    "    pixels_count = 16\r\n",
    "    resized_image = cv2.resize(image, (pixels_count, pixels_count), interpolation=cv2.INTER_LINEAR)\r\n",
    "    return cv2.resize(resized_image, (width, height), interpolation=cv2.INTER_NEAREST)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "And one more function to process inference results of the face detection model. Use the `blur` function to blur a part of image with a face."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def blur_postprocessing(face_detection_inference_result: np.ndarray, original_image: np.ndarray) -> np.ndarray:\r\n",
    "    original_image_height, original_image_width,  _ = original_image.shape\r\n",
    "    processed_image = original_image.copy()\r\n",
    "    \r\n",
    "    detected_faces = parse_face_detection_results(face_detection_inference_result, original_image_width, original_image_height)\r\n",
    "\r\n",
    "    for detected_face in detected_faces:\r\n",
    "        xmin, ymin, xmax, ymax, _ = detected_face\r\n",
    "        face = original_image[ymin:ymax, xmin:xmax]\r\n",
    "        processed_image[ymin:ymax, xmin:xmax] = blur(face)\r\n",
    "            \r\n",
    "    return processed_image"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then prepare a function that prepares the image for inference (use `pre_process_input_image`) and runs inference of the image using  `face_detection_executable_network`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def face_detection_indeference(image: np.ndarray)-> np.ndarray:\r\n",
    "    # 1. Prepare the image\r\n",
    "    input_frame = pre_process_input_image(image, target_width=face_detection_input_width, target_height=face_detection_input_height)\r\n",
    "\r\n",
    "    # 2. Infer the model\r\n",
    "    face_detection_inference_results = face_detection_executable_network.infer(inputs={face_detection_input_blob: input_frame})  \r\n",
    "    return face_detection_inference_results[face_detection_output_blob]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "inference_result = face_detection_indeference(original_image)\r\n",
    "\r\n",
    "# 3. Blur faces on image\r\n",
    "processed_image = blur_postprocessing(inference_result, original_image)\r\n",
    "\r\n",
    "show_images(processed_image)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 2: Add blurring logic to pre-defined video processor"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "After completing the inference of a single image, our next step is to process all frames of a video."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "input_video_stream = cv2.VideoCapture(input_video_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def prepare_output_video_stream(input_video_stream: cv2.VideoCapture, output_video_file_path: str) -> cv2.VideoWriter:\r\n",
    "    width  = int(input_video_stream.get(3))\r\n",
    "    height = int(input_video_stream.get(4))\r\n",
    "    video_writer = cv2.VideoWriter(output_video_file_path, cv2.VideoWriter_fourcc(*'avc1'), 20, (width, height))\r\n",
    "    return video_writer\r\n",
    "\r\n",
    "output_video_stream = prepare_output_video_stream(input_video_stream, output_video_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def face_detection_indeference(image: np.ndarray)-> np.ndarray:\r\n",
    "    input_frame = pre_process_input_image(image, target_width=face_detection_input_width, target_height=face_detection_input_height)\r\n",
    "\r\n",
    "    # 2. Infer the model\r\n",
    "    face_detection_inference_results = face_detection_executable_network.infer(inputs={face_detection_input_blob: input_frame})  \r\n",
    "    return face_detection_inference_results[face_detection_output_blob]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "while input_video_stream.isOpened():\r\n",
    "    # 1. Read the next frame from the input video \r\n",
    "    return_code, original_frame = input_video_stream.read()\r\n",
    "    if not return_code:\r\n",
    "        break\r\n",
    "        \r\n",
    "    inference_result = face_detection_indeference(original_frame)\r\n",
    "\r\n",
    "    # 3. Blur faces on image\r\n",
    "    processed_image = blur_postprocessing(inference_result, original_frame)\r\n",
    "    \r\n",
    "    # 3. Write the resulting frame to the output stream\r\n",
    "    output_video_stream.write(processed_image)\r\n",
    "    \r\n",
    "\r\n",
    "input_video_stream.release()\r\n",
    "# Save the resulting video\r\n",
    "output_video_stream.release()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Show a source video\r\n",
    "HTML(f\"\"\"<video width=\"600\" height=\"400\" controls><source src=\"{output_video_path}\" type=\"video/mp4\"></video>\"\"\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 3: Replace each face on the photo with a smile with corresponding emotion"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 1: Download a Pretrained Model from the Open Model Zoo\r\n",
    "\r\n",
    "Let's download the `emotions-recognition-retail-0003` model first."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!python3 ~/intel/openvino_2021/deployment_tools/open_model_zoo/tools/downloader/downloader.py --name emotions-recognition-retail-0003 --precision FP16 --output_dir data/model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Model IR files\r\n",
    "emotion_recognition_model_xml = 'data/model/intel/emotions-recognition-retail-0003/FP16/emotions-recognition-retail-0003.xml'\r\n",
    "emotion_recognition_model_bin = 'data/model/intel/emotions-recognition-retail-0003/FP16/emotions-recognition-retail-0003.bin'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# call ie_core.read_network to read the OpenVINO IR model\r\n",
    "emotion_recognition_network = ie_core.read_network(emotion_recognition_model_xml, emotion_recognition_model_bin)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 2: Load the network to a device\r\n",
    "\r\n",
    "Use the instance of `IECore`.\r\n",
    "The class `IECore` has a special function called `load_network`, which loads a network to a device.\r\n",
    "This function prepares the network for the first inference on the device \r\n",
    "and returns an instance of the network prepared for an inference (execution). \r\n",
    "This function has many parameters, but in this case, you need to know only about two of them:\r\n",
    "* `network` - instance of `IENetwork`\r\n",
    "* `device_name` - string, contains a device name to infer a model on: CPU, GPU and so on."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "emotion_recognition_network_loaded_on_device = ie_core.load_network(emotion_recognition_network, device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "emotion_recognition_input_layer = next(iter(emotion_recognition_network.input_info))\r\n",
    "emotion_recognition_input_blob = emotion_recognition_network.input_info[emotion_recognition_input_layer].input_data\r\n",
    "\r\n",
    "print(f'Input layer of the emotions-recognition-retail-0003 is {emotion_recognition_input_layer}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "emotion_recognition_input_batch, emotion_recognition_input_channels, emotion_recognition_input_height, emotion_recognition_input_width = emotion_recognition_input_blob.shape\r\n",
    "\r\n",
    "print(f'Input shape of the emotion recognition network is n={emotion_recognition_input_batch}, c={emotion_recognition_input_channels}, h={emotion_recognition_input_height}, w={emotion_recognition_input_width}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "emotion_recognition_output_layer = next(iter(emotion_recognition_network.outputs))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 3: Prepare a frame and run inference"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def emotion_recognition_inference(face_frame: np.ndarray):\r\n",
    "    prepared_frame = pre_process_input_image(face_frame, target_width=emotion_recognition_input_width, target_height=emotion_recognition_input_height)\r\n",
    "    \r\n",
    "    # Run the inference as you have done earlier\r\n",
    "    inference_results = emotion_recognition_network_loaded_on_device.infer({\r\n",
    "        emotion_recognition_input_layer: prepared_frame\r\n",
    "    })\r\n",
    "    \r\n",
    "    # For better understanding of the inference result for this model, read the documentation \r\n",
    "    # https://docs.openvinotoolkit.org/latest/_models_intel_emotions_recognition_retail_0003_description_emotions_recognition_retail_0003.html\r\n",
    "    return inference_results[emotion_recognition_output_layer]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "original_frame = load_input_image('./data/emotion.jpg')\r\n",
    "\r\n",
    "# 2. Infer the model\r\n",
    "face_detection_inference_results = emotion_recognition_inference(original_frame)\r\n",
    "\r\n",
    "face_detection_inference_result = face_detection_inference_results.flatten()\r\n",
    "emotions = ['neutral', 'happy', 'sad', 'surprise', 'anger']\r\n",
    "\r\n",
    "show_images(original_frame)\r\n",
    "\r\n",
    "print('Inference results:')\r\n",
    "for index, prediction in enumerate(face_detection_inference_result):\r\n",
    "    emotion = emotions[index]\r\n",
    "    print(f'{emotion}:\\t{prediction}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 4: Draw boxes and emotions in the frame"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load the image\r\n",
    "original_image = load_input_image(input_image_path)\r\n",
    "original_image_height, original_image_width, *_ = original_image.shape\r\n",
    "\r\n",
    "# Display the input image\r\n",
    "print(\"Input image:\")\r\n",
    "show_images(original_image)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_smile_by_index(emotion_inference_result: np.ndarray) -> np.ndarray:\r\n",
    "    emotions = ['neutral', 'happy', 'sad', 'surprise', 'anger']\r\n",
    "    emotion_index = np.argmax(emotion_inference_result.flatten()) \r\n",
    "    smile_path = f'./data/{emotions[emotion_index]}.png'\r\n",
    "    return cv2.imread(smile_path, -1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def emotion_recognition_inference_postprocess(image: np.ndarray, recognized_emotions: np.ndarray, xmin:int, ymin:int, xmax:int, ymax:int):\r\n",
    "    # Put the title to a frame\r\n",
    "    width = xmax - xmin\r\n",
    "    height = ymax - ymin\r\n",
    "    \r\n",
    "    smile = get_smile_by_index(recognized_emotions)\r\n",
    "    resized_smile = cv2.resize(smile, (width, height))\r\n",
    "\r\n",
    "    alpha_s = resized_smile[:, :, 3] / 255.0\r\n",
    "    alpha_l = 1.0 - alpha_s\r\n",
    "    for c in range(0, 3):\r\n",
    "        image[ymin:ymax, xmin:xmax, c] = (alpha_s * resized_smile[:, :, c] + alpha_l * image[ymin:ymax, xmin:xmax, c])\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "processed_image = original_image.copy()\r\n",
    "face_detection_inference_results = face_detection_indeference(original_image)\r\n",
    "original_image_height, original_image_width, _ = original_image.shape\r\n",
    "\r\n",
    "faces_coordinates = parse_face_detection_results(face_detection_inference_results, original_image_width, original_image_height)\r\n",
    "\r\n",
    "for face_coordinates in faces_coordinates:\r\n",
    "    xmin, ymin, xmax, ymax, confidence = face_coordinates\r\n",
    "    face = original_image[ymin:ymax, xmin:xmax]\r\n",
    "\r\n",
    "    emotion_predictions = emotion_recognition_inference(face)\r\n",
    "    \r\n",
    "    emotion_recognition_inference_postprocess(processed_image, emotion_predictions, xmin, ymin, xmax, ymax)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "show_images(processed_image)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 5: Loop over frames in the input video"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "input_video_stream = cv2.VideoCapture(input_video_path)\r\n",
    "\r\n",
    "original_video_width = int(input_video_stream.get(3))\r\n",
    "original_video_height = int(input_video_stream.get(4))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "output_video_stream = prepare_output_video_stream(input_video_stream, output_video_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "while input_video_stream.isOpened():\r\n",
    "    # 1. Read the next frame from the input video \r\n",
    "    finish, original_frame = input_video_stream.read()\r\n",
    "    if not finish:\r\n",
    "        break\r\n",
    "        \r\n",
    "    # 2. Apply face replacement from previous step\r\n",
    "    face_detection_inference_results = face_detection_indeference(original_frame)\r\n",
    "    \r\n",
    "    faces_coordinates = parse_face_detection_results(face_detection_inference_results, original_video_width, original_video_height)\r\n",
    "\r\n",
    "    for face_coordinates in faces_coordinates:\r\n",
    "\r\n",
    "        xmin, ymin, xmax, ymax, _ = face_coordinates\r\n",
    "        face = original_frame[ymin:ymax, xmin:xmax]\r\n",
    "\r\n",
    "        emotion_predictions = emotion_recognition_inference(face)\r\n",
    "\r\n",
    "        emotion_recognition_inference_postprocess(original_frame, emotion_predictions, xmin, ymin, xmax, ymax)\r\n",
    "    # 3. Write the resulting frame to the output stream\r\n",
    "    output_video_stream.write(original_frame)\r\n",
    "    \r\n",
    "input_video_stream.release()\r\n",
    "# Save the resulting video\r\n",
    "output_video_stream.release()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Show a source video\r\n",
    "HTML(f\"\"\"<video width=\"600\" height=\"400\" controls><source src=\"{output_video_path}\" type=\"video/mp4\"></video>\"\"\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}