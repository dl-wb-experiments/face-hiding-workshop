{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the required modules\n",
    "import cv2\n",
    "import time\n",
    "import PIL.Image\n",
    "from io import BytesIO\n",
    "import IPython.display\n",
    "import numpy as np\n",
    "\n",
    "# Import the Inference Engine\n",
    "from openvino.inference_engine import IECore, IENetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_XML = '/Users/demidovs/Documents/Work/repositories/workbench/wb/data/models/14/original/face-detection-adas-0001.xml'\n",
    "MODEL_BIN = '/Users/demidovs/Documents/Work/repositories/workbench/wb/data/models/14/original/face-detection-adas-0001.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare IE\n",
    "ie_core = IECore()\n",
    "\n",
    "network = ie_core.read_network(MODEL_XML, MODEL_BIN)\n",
    "network_input_name = next(iter(network.input_info))\n",
    "network_input_blob = network.input_info[network_input_name].input_data\n",
    "batch, channels, input_layer_h, input_layer_w = network_input_blob.shape\n",
    "\n",
    "print(f'Input shape of the network is [{batch}, {channels}, {input_layer_h}, {input_layer_w}]')\n",
    "\n",
    "network_output_blob = next(iter(network.outputs))\n",
    "print(f'Network outputs: {network_output_blob}')\n",
    "\n",
    "DEVICE = 'CPU'\n",
    "network_loaded_to_device = ie_core.load_network(network, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "def pre_process_frame_to_network_input(input_frame: np.ndarray, batch: int, channels: int, input_layer_height: int, input_layer_width: int) -> np.ndarray:\n",
    "    # Resize the frame to the network input \n",
    "    resized_frame = cv2.resize(input_frame, (input_layer_width, input_layer_height))\n",
    "    \n",
    "    # Change the data layout from HWC to CHW\n",
    "    transposed_frame = resized_frame.transpose((2, 0, 1))  \n",
    "    \n",
    "    # Reshape the frame to the network input \n",
    "    reshaped_frame = transposed_frame.reshape((batch, channels, input_layer_height, input_layer_width))\n",
    "    \n",
    "    return reshaped_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def face_detection_inference(input_frame: np.ndarray) -> np.ndarray:\n",
    "    feed_dict = {\n",
    "        network_input_name: input_frame\n",
    "    }\n",
    "    \n",
    "    # All is ready for the main thing - inference!\n",
    "    # You have read and loaded the network to the device, prepared input data and now you are ready to infer.\n",
    "    \n",
    "    # Step 11:\n",
    "    # To start an inference, call the `infer` function of the `network_loaded_to_device` variable. \n",
    "    # We must set input data (a dictionary).\n",
    "    inference_result = network_loaded_to_device.infer(feed_dict)\n",
    "    \n",
    "    # Great! The `inference_result` variable contains output data after inference of the network.\n",
    "    # `inference_result` is a dictionary, \n",
    "    #  where key is the name of the output name, \n",
    "    #        value is data from the blob.\n",
    "    \n",
    "    return inference_result[network_output_blob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _add_confidence_label(original_frame: np.ndarray, confidence: float, coordinates):\n",
    "    # Draw a box and a label\n",
    "    color = (0, 255, 0)\n",
    "    \n",
    "    # Create the title of an object\n",
    "    text = f'{round(confidence * 100, 2)}%'\n",
    "    \n",
    "    # Put the title to a frame\n",
    "    cv2.putText(original_frame, text, coordinates, cv2.FONT_HERSHEY_COMPLEX, 2, color, 2)\n",
    "\n",
    "    \n",
    "def _visualize_detection(original_frame: np.ndarray, bbox):\n",
    "    xmin, ymin, xmax, ymax = bbox\n",
    "    face_region = original_frame[ymin:ymax, xmin:xmax]\n",
    "    \n",
    "    if face_region.size == 0:\n",
    "        return\n",
    "    \n",
    "    # original_frame[ymin:ymax, xmin:xmax] = _blur_region(face_region)\n",
    "    original_frame[ymin:ymax, xmin:xmax] = _pixelize_region(face_region)\n",
    "\n",
    "\n",
    "def _blur_region(region: np.ndarray) -> np.ndarray:\n",
    "    return cv2.GaussianBlur(region, (23, 23), 50)\n",
    "\n",
    "\n",
    "def _pixelize_region(region: np.ndarray) -> np.ndarray:\n",
    "    height, width = region.shape[:2]\n",
    "    pixels_count = 16\n",
    "    temp = cv2.resize(region, (pixels_count, pixels_count), interpolation=cv2.INTER_LINEAR)\n",
    "    return cv2.resize(temp, (width, height), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "\n",
    "def process_detection(original_frame: np.ndarray, detection: np.ndarray):       \n",
    "    confidence =  detection[2]\n",
    "    if confidence < 0.3:\n",
    "        return\n",
    "    frame_h, frame_w = original_frame.shape[:2]\n",
    "    xmin = int(detection[3] * frame_w)\n",
    "    ymin = int(detection[4] * frame_h)\n",
    "    xmax = int(detection[5] * frame_w)\n",
    "    ymax = int(detection[6] * frame_h)\n",
    "    \n",
    "    _add_confidence_label(original_frame, confidence, coordinates=(xmin, ymin - 7))\n",
    "    \n",
    "    _visualize_detection(original_frame, bbox=(xmin, ymin, xmax, ymax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "\n",
    "# Use 'jpeg' instead of 'png' (~5 times faster)\n",
    "def array_to_image(array: np.ndarray, format: str = 'jpeg'):\n",
    "    binary_stream = BytesIO()\n",
    "    PIL.Image.fromarray(array).save(binary_stream, format)\n",
    "    return IPython.display.Image(data=binary_stream.getvalue())\n",
    "\n",
    "\n",
    "def get_frame(video_capture: cv2.VideoCapture) -> np.ndarray:\n",
    "    _, frame = video_capture.read()\n",
    "    \n",
    "    # Flip image for natural viewing\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "video_capture = cv2.VideoCapture(0)\n",
    "try:\n",
    "    width  = video_capture.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "    height = video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "\n",
    "    video_display = IPython.display.display('', display_id=1)\n",
    "    fps_label_display = IPython.display.display('', display_id=2)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            t1 = time.time()\n",
    "            frame = get_frame(video_capture)\n",
    "\n",
    "            pre_processed_frame = pre_process_frame_to_network_input(frame, batch, channels, input_layer_h, input_layer_w)\n",
    "\n",
    "            inference_result = face_detection_inference(pre_processed_frame)\n",
    "\n",
    "            for detected_face in inference_result[0][0]:\n",
    "                process_detection(frame, detected_face)\n",
    "\n",
    "\n",
    "            image = array_to_image(frame)\n",
    "            video_display.update(image)\n",
    "\n",
    "            t2 = time.time()\n",
    "\n",
    "            s = f\"\"\"{int(1/(t2-t1))} FPS\"\"\"\n",
    "            fps_label_display.update( IPython.display.HTML(s) )\n",
    "        except KeyboardInterrupt:\n",
    "            print()\n",
    "            IPython.display.clear_output()\n",
    "            print (\"Stream stopped\")\n",
    "            break\n",
    "finally:\n",
    "    video_capture.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
